{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DenseDepth1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"41UChf-ZNQIT","colab_type":"code","colab":{}},"source":["a = []\n","while(1):\n","    a.append('1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCzRZyiVJc1S","colab_type":"code","outputId":"65cc3173-0f21-4413-be08-e8e279760618","executionInfo":{"status":"ok","timestamp":1589100390302,"user_tz":-330,"elapsed":4718,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun May 10 08:46:26 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f2Jq86Ty9ObE","colab_type":"code","outputId":"da9aa6b6-8342-4394-85c5-9d882e5ba027","executionInfo":{"status":"ok","timestamp":1589100420387,"user_tz":-330,"elapsed":19463,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rUnAd3STkDSG","colab_type":"code","outputId":"cc16fe04-aab8-490d-9f5f-13ee85bc1e44","executionInfo":{"status":"ok","timestamp":1589025839997,"user_tz":-330,"elapsed":5932,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# %cd /content/gdrive/My\\ Drive/Colab Notebooks/DenseDepth-master\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ahkR4C5dEnR0","colab_type":"code","outputId":"bdace310-f8b1-4856-863d-0d7c70fa0824","executionInfo":{"status":"ok","timestamp":1589100424666,"user_tz":-330,"elapsed":5807,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!git clone https://github.com/ialhashim/DenseDepth.git"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'DenseDepth'...\n","remote: Enumerating objects: 235, done.\u001b[K\n","remote: Total 235 (delta 0), reused 0 (delta 0), pack-reused 235\u001b[K\n","Receiving objects: 100% (235/235), 11.80 MiB | 12.86 MiB/s, done.\n","Resolving deltas: 100% (115/115), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fFQgwMlNExak","colab_type":"code","outputId":"97d4b2e1-da3c-4cb6-b7fa-ef60e5ff3ac2","executionInfo":{"status":"ok","timestamp":1589100428997,"user_tz":-330,"elapsed":9591,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["!wget https://s3-eu-west-1.amazonaws.com/densedepth/nyu.h5 -O ./DenseDepth/nyu.h5"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-05-10 08:47:04--  https://s3-eu-west-1.amazonaws.com/densedepth/nyu.h5\n","Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.88.235\n","Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.88.235|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 172897376 (165M) [application/h5]\n","Saving to: ‘./DenseDepth/nyu.h5’\n","\n","./DenseDepth/nyu.h5 100%[===================>] 164.89M  95.5MB/s    in 1.7s    \n","\n","2020-05-10 08:47:06 (95.5 MB/s) - ‘./DenseDepth/nyu.h5’ saved [172897376/172897376]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AiJKd6uLE9Gr","colab_type":"code","outputId":"3252a52c-5cf6-40fe-9931-ae9ca4336bdb","executionInfo":{"status":"ok","timestamp":1589100428999,"user_tz":-330,"elapsed":4384,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd /content/DenseDepth \n","\n","# !cp /content/drive/My\\ Drive/data/bg_fg_images1.zip Data/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/DenseDepth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hW_LC2wIQzXl","colab_type":"code","colab":{}},"source":["# from google.colab import files\n","\n","# uploaded = files.upload()\n","\n","# for fn in uploaded.keys():\n","#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","#       name=fn, length=len(uploaded[fn])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvqoJory39pK","colab_type":"code","colab":{}},"source":["import zipfile\n","\n","def files():\n","    count = 0\n","    try:\n","        with zipfile.ZipFile('/content/drive/My Drive/data/bg_fg_images_str.zip', mode = 'r') as file:\n","          file.extractall('/content/DenseDepth/Data/')\n","    except zipfile.LargeZipFile: # it raises an 'LargeZipFile' error because you didn't enable the 'Zip64'\n","        print('Error: File size if too large')\n","\n","files()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-D6akcp5Jw64","colab_type":"code","colab":{}},"source":["# import os\n","# cnt = 0\n","# for i in os.listdir('/content/DenseDepth/Data/F2/bgScaled_0'):\n","#   print(i)\n","\n","# # print(cnt)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1RVSN9qnJwnM","colab_type":"code","colab":{}},"source":["# for i in os.listdir()\n","# import torch\n","import matplotlib.pyplot as plt\n","import skimage\n","from skimage.transform import resize\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Tv4ovGw0Pcu","colab_type":"code","outputId":"b9e01a54-da59-42cf-ee8e-d4c230f3cd94","executionInfo":{"status":"ok","timestamp":1589041730925,"user_tz":-330,"elapsed":646411,"user":{"displayName":"Pratik Jain","photoUrl":"","userId":"13982555607852599086"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["folder_name = 'bgScaled_24'\n","in_path = '/content/DenseDepth/Data/' + folder_name + '/*.jpg'\n","print(in_path)\n","# !python test_new.py --input '/content/DenseDepth/Data/F2/*.jpg'\n","# !python test_new.py --input in_path\n","!python test_new.py --input '/content/DenseDepth/Data/bgScaled_24/*.jpg'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/DenseDepth/Data/bgScaled_24/*.jpg\n","Using TensorFlow backend.\n","Loading model...\n","\n","Model loaded (nyu.h5).\n","cnt 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c0t9yyK2Fh2x","colab_type":"code","colab":{}},"source":["!pip install -U -q PyDrive\n","\n","from google.colab import files\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import zipfile\n","import os\n","import sys\n","import zipfile\n","import os\n","import sys\n","# folder_name = 'bgScaled_0'\n","zipname = folder_name\n","\n","def zipfolder(foldername, target_dir):            \n","    zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n","    rootlen = len(target_dir)\n","    for base, dirs, files in os.walk(target_dir):\n","        for file in files:\n","            fn = os.path.join(base, file)\n","            zipobj.write(fn, fn[rootlen:])\n","\n","zipfolder(zipname, '/content/DenseDepth/results/' + zipname + '/')\n","\n","'/content/DenseDepth/Data/results/' + zipname + '/'\n","\n","################################################\n","\n","file_name = folder_name + \".zip\"\n","# file_name\n","from googleapiclient.http import MediaFileUpload\n","from googleapiclient.discovery import build\n","\n","auth.authenticate_user()\n","drive_service = build('drive', 'v3')\n","\n","def save_file_to_drive(name, path):\n","  file_metadata = {'name': name, 'mimeType': 'application/octet-stream'}\n","  media = MediaFileUpload(path, mimetype='application/octet-stream', resumable=True)\n","  created = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n","  \n","  return created\n","\n","save_file_to_drive(file_name, file_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayZzNzDOFhMw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ra7GEtQBHUMS","colab_type":"code","colab":{}},"source":["# !rm -rf '/content/DenseDepth/Data/File1/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIkhaphk39U-","colab_type":"code","outputId":"bc7b8a91-4809-4da0-8968-3ffc9bd5b812","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# !unzip '/content/drive/My Drive/bg_fg_images1.zip' -d '/content/DenseDepth/Data/'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/bg_fg_images1.zip\n","warning [/content/drive/My Drive/bg_fg_images1.zip]:  1216555522 extra bytes at beginning or within zipfile\n","  (attempting to process anyway)\n","error [/content/drive/My Drive/bg_fg_images1.zip]:  start of central directory not found;\n","  zipfile corrupt.\n","  (please check that you have transferred or created the zipfile in the\n","  appropriate BINARY mode and that you have compiled UnZip properly)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yCkGKiqi39QR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5na61e-Y39Kh","colab_type":"code","colab":{}},"source":["################ test_new.py ##################\n","\n","# import os\n","# import glob\n","# import argparse\n","# import matplotlib\n","# import numpy as np\n","# from PIL import Image\n","# import torch\n","# # Keras / TensorFlow\n","# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n","# from keras.models import load_model\n","# from layers import BilinearUpSampling2D\n","# from utils import predict, load_images, display_images\n","# from matplotlib import pyplot as plt\n","\n","# # Argument Parser\n","# parser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\n","# parser.add_argument('--model', default='nyu.h5', type=str, help='Trained Keras model file.')\n","# parser.add_argument('--input', default='examples/*.png', type=str, help='Input filename or folder.')\n","# args = parser.parse_args()\n","\n","# # Custom object needed for inference and training\n","# custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\n","\n","# print('Loading model...')\n","\n","# # Load model into GPU / CPU\n","# model = load_model(args.model, custom_objects=custom_objects, compile=False)\n","\n","# print('\\nModel loaded ({0}).'.format(args.model))\n","\n","# # Input images\n","# #inputs,ifiles = load_images( glob.glob(args.input), model )\n","\n","# def load_images1(image_files, model, ofdr):\n","#     from skimage.transform import resize\n","#     loaded_images = []\n","#     fnames = []\n","#     cnt = 0\n","#     for file in image_files:\n","#         cnt = cnt + 1\n","#         fnames.append(file)\n","#         x = np.clip(np.asarray(Image.open( file ), dtype=float) / 255, 0, 1)\n","#         x = resize(x, (448,448), order=1, preserve_range=True, mode='reflect', anti_aliasing=True )\n","#         loaded_images.append(x)\n","\n","#         # if cnt % 10 ==0:\n","#         print('cnt',cnt)\n","#         inputs = np.stack(loaded_images, axis=0)\n","#         # print('\\nLoaded ({0}) images of size {1}.'.format(inputs.shape[0], inputs.shape[1:]))\n","\n","#         outputs = predict(model, inputs)\n","#         # print('Prediction Done')\n","\n","#         display_images(outputs.copy(), fnames , inputs.copy(), ofdr = ofdr)\n","#         # print('write done')\n","#         loaded_images.clear()\n","#         fnames.clear()\n","#         del outputs, inputs\n","#         if cnt % 1000 ==0:\n","#           torch.cuda.empty_cache()\n","\n","\n","\n","# load_images1( glob.glob(args.input), model, 'bgScaled_0')\n","\n","# # Compute results\n","# #outputs = predict(model, inputs)\n","\n","# #matplotlib problem on ubuntu terminal fix\n","# #matplotlib.use('TkAgg')   \n","\n","# # Display results\n","# #display_images(outputs.copy(), ifiles, inputs.copy())\n","\n","# #plt.figure(figsize=(10,5))\n","# #plt.imshow(viz)\n","# #plt.savefig('test.png')\n","# #plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ieIa3x3DilV","colab_type":"code","colab":{}},"source":["# ############# utils.py #################\n","\n","# import numpy as np\n","# from PIL import Image\n","# import os\n","\n","# def DepthNorm(x, maxDepth):\n","#     return maxDepth / x\n","\n","# def predict(model, images, minDepth=10, maxDepth=5000, batch_size=2):\n","#     # Support multiple RGBs, one RGB image, even grayscale \n","#     if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n","#     if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n","#     # Compute predictions\n","#     predictions = model.predict(images, batch_size=batch_size)\n","#     # Put in expected range\n","#     return np.clip(DepthNorm(predictions, maxDepth=maxDepth), minDepth, maxDepth) / maxDepth\n","\n","# def scale_up(scale, images):\n","#     from skimage.transform import resize\n","#     scaled = []\n","    \n","#     for i in range(len(images)):\n","#         img = images[i]\n","#         output_shape = (scale * img.shape[0], scale * img.shape[1])\n","#         scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode='reflect', anti_aliasing=True ) )\n","\n","#     return np.stack(scaled)\n","\n","# def load_images(image_files):\n","#     from skimage.transform import resize\n","#     loaded_images = []\n","#     fnames = []\n","#     for file in image_files:\n","#         print('file',file)\n","#         x = np.clip(np.asarray(Image.open( file ), dtype=float) / 255, 0, 1)\n","#         #x = resize(x, (960,960), order=1, preserve_range=True, mode='reflect', anti_aliasing=True )\n","#         loaded_images.append(x)\n","#     return np.stack(loaded_images, axis=0), image_files\n","\n","# def to_multichannel(i):\n","#     if i.shape[2] == 3: return i\n","#     i = i[:,:,0]\n","#     return np.stack((i,i,i), axis=2)\n","        \n","# def display_images(outputs, ifiles, inputs=None, ofdr = None, gt=None, is_colormap=True, is_rescale=True):\n","#     import matplotlib.pyplot as plt\n","#     import skimage\n","#     from skimage.transform import resize\n","#     fig = plt.figure(figsize=(2.24,2.24))\n","#     ax = plt.Axes(fig, [0.,0.,1.,1.])\n","#     ax.set_axis_off()\n","#     fig.add_axes(ax)\n","\n","#     plasma = plt.get_cmap('gray')\n","\n","#     shape = (outputs[0].shape[0], outputs[0].shape[1], 3)\n","\n","#     all_images = []\n","\n","#     for i in range(outputs.shape[0]):\n","#         imgs = []\n","        \n","#         if is_colormap:\n","#             rescaled = outputs[i][:,:,0]\n","#             if is_rescale:\n","#                 rescaled = rescaled - np.min(rescaled)\n","#                 rescaled = rescaled / np.max(rescaled)\n","#             imgs.append(plasma(rescaled)[:,:,:3])\n","#         else:\n","#             imgs.append(to_multichannel(outputs[i]))\n","\n","#         ax.imshow(plasma(rescaled)[:,:,:3],aspect='equal')\n","#         aa = '/content/DenseDepth/results/' + ofdr + '/'\n","#         if not os.path.isdir(aa):\n","#            os.mkdir(aa)\n","#         plt.savefig(aa + os.path.basename(ifiles[i]))\n","\n","#         img_set = np.hstack(imgs)\n","#         all_images.append(img_set)\n","\n","#     all_images = np.stack(all_images)\n","    \n","#     #return skimage.util.montage(all_images, multichannel=True, fill=(0,0,0))\n","\n","# def save_images(filename, outputs, inputs=None, gt=None, is_colormap=True, is_rescale=False):\n","#     montage =  display_images(outputs, inputs, is_colormap, is_rescale)\n","#     im = Image.fromarray(np.uint8(montage*255))\n","#     im.save(filename)\n","\n","# def load_test_data(test_data_zip_file='nyu_test.zip'):\n","#     print('Loading test data...', end='')\n","#     import numpy as np\n","#     from data import extract_zip\n","#     data = extract_zip(test_data_zip_file)\n","#     from io import BytesIO\n","#     rgb = np.load(BytesIO(data['eigen_test_rgb.npy']))\n","#     depth = np.load(BytesIO(data['eigen_test_depth.npy']))\n","#     crop = np.load(BytesIO(data['eigen_test_crop.npy']))\n","#     print('Test data loaded.\\n')\n","#     return {'rgb':rgb, 'depth':depth, 'crop':crop}\n","\n","# def compute_errors(gt, pred):\n","#     thresh = np.maximum((gt / pred), (pred / gt))\n","#     a1 = (thresh < 1.25   ).mean()\n","#     a2 = (thresh < 1.25 ** 2).mean()\n","#     a3 = (thresh < 1.25 ** 3).mean()\n","#     abs_rel = np.mean(np.abs(gt - pred) / gt)\n","#     rmse = (gt - pred) ** 2\n","#     rmse = np.sqrt(rmse.mean())\n","#     log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\n","#     return a1, a2, a3, abs_rel, rmse, log_10\n","\n","# def evaluate(model, rgb, depth, crop, batch_size=6, verbose=False):\n","#     N = len(rgb)\n","\n","#     bs = batch_size\n","\n","#     predictions = []\n","#     testSetDepths = []\n","    \n","#     for i in range(N//bs):    \n","#         x = rgb[(i)*bs:(i+1)*bs,:,:,:]\n","        \n","#         # Compute results\n","#         true_y = depth[(i)*bs:(i+1)*bs,:,:]\n","#         pred_y = scale_up(2, predict(model, x/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n","        \n","#         # Test time augmentation: mirror image estimate\n","#         pred_y_flip = scale_up(2, predict(model, x[...,::-1,:]/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\n","\n","#         # Crop based on Eigen et al. crop\n","#         true_y = true_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n","#         pred_y = pred_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n","#         pred_y_flip = pred_y_flip[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\n","        \n","#         # Compute errors per image in batch\n","#         for j in range(len(true_y)):\n","#             predictions.append(   (0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j]))   )\n","#             testSetDepths.append(   true_y[j]   )\n","\n","#     predictions = np.stack(predictions, axis=0)\n","#     testSetDepths = np.stack(testSetDepths, axis=0)\n","\n","#     e = compute_errors(predictions, testSetDepths)\n","\n","#     if verbose:\n","#         print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n","#         print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\n","\n","#     return e\n"],"execution_count":0,"outputs":[]}]}